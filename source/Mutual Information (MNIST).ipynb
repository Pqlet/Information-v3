{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information (MNIST)\n",
    "\n",
    "Эксперименты с оценкой энтропии для данных рукописных цифр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преамбула"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81zMD0EitGlJ"
   },
   "source": [
    "### Math, Numpy, Scipy, Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TrbCI8-s4re"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "import scipy.linalg as spl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GwNnxNiDgYw"
   },
   "source": [
    "### Matplotlib, Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0SjY2FyDgiY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWH6JIAJtbs3"
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rb9G9YxtfD3"
   },
   "outputs": [],
   "source": [
    "# Метод главных компонент.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Выбор модели по кросс-валидации (поиск по сетке).\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVaeOz9tbwI"
   },
   "source": [
    "### Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R5GjsuMtPe4"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "global_n_jobs = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCTOH5CQuULh"
   },
   "source": [
    "### OS, shutil, Json, CSV, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwC7bZldt4qY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutinfo.estimators.mutual_information import MutualInfoEstimator\n",
    "from mutinfo.keras.layers import TunableGaussianNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9zP5A4nufLs"
   },
   "source": [
    "## Вспомогательное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Глобальная информация.\n",
    "global_info = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flPbS2ebuY7h"
   },
   "outputs": [],
   "source": [
    "# Информация об опыте.\n",
    "info = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RyvEh9bulAV"
   },
   "outputs": [],
   "source": [
    "def normalize_uint8(data, label):\n",
    "    \"\"\"Нормализация: `uint8` -> `float32`.\"\"\"\n",
    "    return tf.cast(data, tf.float32) / 255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5xA0W2DvIBq"
   },
   "outputs": [],
   "source": [
    "def imshow_array(array):\n",
    "    \"\"\"Отображение массива нормированных пикселей.\"\"\"\n",
    "    plt.axis('off')\n",
    "    plt.imshow((255.0 * array).astype(np.uint8), cmap=plt.get_cmap(\"gray\"), vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5OjeuhXvP6O"
   },
   "outputs": [],
   "source": [
    "def dataset_Y_to_X(X, Y):\n",
    "    \"\"\"Поменять у датасета пары (X, Y) на (X, X) (нужно, например, для обучения автоэнкодера).\"\"\"\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo0MXHDowRLZ"
   },
   "outputs": [],
   "source": [
    "def concave_loss(y_true, y_pred):\n",
    "    \"\"\"Вогнутая функция потерь, дающая более четкие изображения при обучении.\"\"\"\n",
    "    delta = tf.keras.backend.abs(y_true - y_pred)\n",
    "    squared = tf.keras.backend.square(y_true - y_pred)\n",
    "    return tf.keras.backend.mean(delta - 0.5 * squared, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Путь к папке с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tneJ2JaEwztO"
   },
   "outputs": [],
   "source": [
    "#path = \"/content/drive/My Drive/Information_v2/\"\n",
    "path = os.path.abspath(os.getcwd()) + \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = path + \"mutual_information/MNIST/\"\n",
    "models_path = experiments_path + \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полный набор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_full_train, ds_full_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir=path + 'tensorflow_datasets/',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full_train = ds_full_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_full_test  = ds_full_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_full_train.take(60000)\n",
    "ds_train = np.array([sample for sample in ds_train])\n",
    "\n",
    "ds_test  = ds_full_test.take(60000)\n",
    "ds_test  = np.array([sample for sample in ds_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_X = ds_train[:,0]\n",
    "ds_test_X  = ds_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_Y = ds_train[:,1]\n",
    "ds_test_Y = ds_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик для изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочные и тестовые наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_ae_train, ds_ae_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir=path + 'tensorflow_datasets/',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ae_train = ds_ae_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_train = ds_ae_train.map(dataset_Y_to_X, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_train = ds_ae_train.cache()\n",
    "ds_ae_train = ds_ae_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_ae_train = ds_ae_train.batch(ae_batch_size)\n",
    "ds_ae_train = ds_ae_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ae_test = ds_ae_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_test = ds_ae_test.map(dataset_Y_to_X, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_ae_test = ds_ae_test.batch(ae_batch_size)\n",
    "ds_ae_test = ds_ae_test.cache()\n",
    "ds_ae_test = ds_ae_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автокодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    "\n",
    "codes_dim_X = 10 # MNSIT\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 1.0)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 12, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 18, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 27, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "\n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tf.keras.layers.Flatten()(next_layer)\n",
    "    next_layer = tf.keras.layers.Dense(dimension)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_layer = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dense(4*4*27)(next_layer)\n",
    "    next_layer = tf.keras.layers.Reshape((4, 4, 27))(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.2)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 18, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.Cropping2D(cropping=((0, 1), (0, 1)))(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 12, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 0 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.GaussianNoise(0.1)(next_layer)\n",
    "    next_layer = tf.keras.layers.UpSampling2D(size=(2, 2))(next_layer)\n",
    "    next_layer = tf.keras.layers.Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.Activation('sigmoid')(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    output_layer = next_layer\n",
    "\n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_layer, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 5e-3)\n",
    "    autoencoder.compile(loss = concave_loss, optimizer = opt, loss_weights = [1.0])\n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_X_autoencoder = True#False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_X_autoencoder:\n",
    "    encoder_X = tf.keras.models.load_model(models_path + \"autoencoder/encoder_X.h5\")\n",
    "    decoder_X = tf.keras.models.load_model(models_path + \"autoencoder/decoder_X.h5\")\n",
    "    autoencoder_X = tf.keras.Sequential([encoder_X, decoder_X])\n",
    "    autoencoder_X.compile(loss = concave_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_X_autoencoder:\n",
    "    encoder_X, decoder_X, autoencoder_X = cnn_autoencoder(mnist_shape, codes_dim_X)\n",
    "    \n",
    "    autoencoder_X.fit(\n",
    "        ds_ae_train,\n",
    "        epochs=300,\n",
    "        validation_data=ds_ae_test\n",
    "    )\n",
    "    \n",
    "    # Сохранение моделей.\n",
    "    autoencoder_X.save(models_path + \"/autoencoder/autoencoder_X.h5\")\n",
    "    encoder_X.save(models_path + \"/autoencoder/encoder_X.h5\")\n",
    "    decoder_X.save(models_path + \"/autoencoder/decoder_X.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификатор изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Номер исследуемого слоя.\n",
    "layer_index = 5\n",
    "\n",
    "# Стандартное отклонение шума, добавляемого к слоям.\n",
    "layers_noise_std = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_epochs = next_epoch - epochs_counter\n",
    "epochs_counter = next_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBi5v6hSAk1K"
   },
   "outputs": [],
   "source": [
    "dataset_path = experiments_path + (\"%.1e\" % layers_noise_std) + \"/\" + \"layer_\" + str(layer_index) + \"/\" + str(epochs_counter) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOu7nB9gx9vH"
   },
   "outputs": [],
   "source": [
    "full_path = dataset_path + \"autoencoders/\"\n",
    "os.makedirs(full_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировочные и тестовые наборы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_batch_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_cl_train, ds_cl_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir=path + 'tensorflow_datasets/',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cl_train = ds_cl_train.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_cl_train = ds_cl_train.cache()\n",
    "ds_cl_train = ds_cl_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_cl_train = ds_cl_train.batch(cl_batch_size)\n",
    "ds_cl_train = ds_cl_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cl_test = ds_cl_test.map(normalize_uint8, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_cl_test = ds_cl_test.batch(cl_batch_size)\n",
    "ds_cl_test = ds_cl_test.cache()\n",
    "ds_cl_test = ds_cl_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_classifier(shape_input):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_0')(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.  \n",
    "    next_layer = tfa.layers.SpectralNormalization(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init\n",
    "        ),\n",
    "        name='SN_1'\n",
    "    )(next_layer)\n",
    "    \n",
    "    #next_layer = tf.keras.layers.BatchNormalization()(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='Dropout_1')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_1')(next_layer)\n",
    "\n",
    "    output_layer_1 = next_layer\n",
    "\n",
    "    # 2 блок слоёв. \n",
    "    next_layer = tfa.layers.SpectralNormalization(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init\n",
    "        ),\n",
    "        name='SN_2'\n",
    "    )(next_layer)\n",
    "    \n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BatchNormalization_2')(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='Dropout_2')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_2')(next_layer)\n",
    "\n",
    "    output_layer_2 = next_layer\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters = 16, kernel_size = (3, 3), strides = (1, 1), padding = 'same', kernel_initializer = init\n",
    "        ),\n",
    "        name='SN_3'\n",
    "    )(next_layer)\n",
    "    \n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BatchNormalization_3')(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1, name='Dropout_3')(next_layer)\n",
    "    next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), padding = 'same')(next_layer)\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_3')(next_layer)\n",
    "\n",
    "    output_layer_3 = next_layer\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Flatten()(next_layer)\n",
    "    \n",
    "    next_layer = tfa.layers.SpectralNormalization(\n",
    "        tf.keras.layers.Dense(16),\n",
    "        name='SN_4'\n",
    "    )(next_layer)\n",
    "    \n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BatchNormalization_4')(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_4')(next_layer)\n",
    "\n",
    "    output_layer_4 = next_layer\n",
    "    \n",
    "    # 5 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(\n",
    "        tf.keras.layers.Dense(10),\n",
    "        name='SN_5'\n",
    "    )(next_layer)\n",
    "    \n",
    "    next_layer = tf.keras.layers.BatchNormalization(name='BatchNormalization_5')(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    next_layer = TunableGaussianNoise(layers_noise_std, name='GaussianNoise_5')(next_layer)\n",
    "\n",
    "    output_layer_5 = next_layer\n",
    "\n",
    "    # Вывод.\n",
    "    #next_layer = tf.keras.layers.Flatten()(next_layer)\n",
    "    next_layer = tf.keras.layers.Dense(10)(next_layer)\n",
    "    output_layer = tf.keras.layers.Activation('softmax')(next_layer)\n",
    "\n",
    "    # Модель.\n",
    "    debug_model = tf.keras.models.Model([input_layer],\n",
    "                                        [output_layer_1,\n",
    "                                         output_layer_2,\n",
    "                                         output_layer_3,\n",
    "                                         output_layer_4,\n",
    "                                         output_layer_5])\n",
    "    model = tf.keras.models.Model(input_layer, output_layer)\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(lr = 1e-3)\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt,\n",
    "                  loss_weights = [1.0], metrics=['accuracy'])\n",
    "    return model, debug_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_X_classifier = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели.\n",
    "if load_X_classifier:\n",
    "    classifier = tf.keras.models.load_model(models_path + \"/classifier/classifier.h5\")\n",
    "    debug_classifier = tf.keras.models.load_model(models_path + \"/classifier/debug_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_X_classifier:\n",
    "    classifier, debug_classifier = convolutional_classifier(mnist_shape)\n",
    "    # Сводка по модели.\n",
    "    classifier.summary()\n",
    "    # Отрисовка модели.\n",
    "    #tf.keras.utils.plot_model(classifier, show_shapes = True, show_layernames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback = classifier.fit(\n",
    "    ds_cl_train,\n",
    "    epochs=delta_epochs,\n",
    "    validation_data=ds_cl_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.array(history_callback.history[\"loss\"])\n",
    "val_loss_history = np.array(history_callback.history[\"val_loss\"])\n",
    "accuracy_history = np.array(history_callback.history[\"accuracy\"])\n",
    "val_accuracy_history = np.array(history_callback.history[\"val_accuracy\"])\n",
    "\n",
    "info['last_loss'] = loss_history[-1]\n",
    "info['last_val_loss'] = val_loss_history[-1]\n",
    "info['last_accuracy'] = accuracy_history[-1]\n",
    "info['last_val_accuracy'] = val_accuracy_history[-1]\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка взаимной информации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение значений слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приходится делать predict по частям.\n",
    "min_batch_number = 10\n",
    "\n",
    "_splitted = tf.split(tf.stack(ds_train_X), min_batch_number)\n",
    "_layer_predicted_train = tf.concat([debug_classifier(_splitted[i])[layer_index - 1] for i in range(min_batch_number)], 0)\n",
    "\n",
    "_splitted = tf.split(tf.stack(ds_test_X), min_batch_number)\n",
    "_layer_predicted_test = tf.concat([debug_classifier(_splitted[i])[layer_index - 1] for i in range(min_batch_number)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_L = np.array([_layer_predicted_train[i].numpy().flatten() for i in range(_layer_predicted_train.shape[0])])\n",
    "ds_test_L  = np.array([_layer_predicted_test[i].numpy().flatten() for i in range(_layer_predicted_test.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2coUXxYyAVx"
   },
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Сжатие данных предлагается делать автокодировщиком.\n",
    "Для архитектуры специфицируется только формат входных данных, а также размерность внутреннего представления (кодов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa1TECWvxuEF"
   },
   "outputs": [],
   "source": [
    "# РАЗМЕРНОСТЬ КОДА.\n",
    "# #\n",
    "# #\n",
    "\n",
    "codes_dim_L = 4  # Слой.\n",
    "\n",
    "# #\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3McJCha_b_G"
   },
   "outputs": [],
   "source": [
    "# Число эпох для обучения.\n",
    "autoencoders_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lwIjSKL_gMv"
   },
   "outputs": [],
   "source": [
    "info['autoencoders_epochs'] = autoencoders_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45W1qKkoB90y"
   },
   "source": [
    "### Автокодировщик для слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2475m-kB1mm"
   },
   "outputs": [],
   "source": [
    "def dense_autoencoder(shape_input, dimension):\n",
    "    # Инициализация весов.\n",
    "    init = tf.keras.initializers.RandomNormal(stddev = 0.02)\n",
    "\n",
    "    # Входные данные генератора / выборки.\n",
    "    input_layer = tf.keras.layers.Input(shape_input)\n",
    "    next_layer = input_layer\n",
    "    next_layer = tf.keras.layers.GaussianNoise(0.02)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(512, kernel_initializer = init),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "\n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(256, kernel_initializer = init),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(128, kernel_initializer = init),\n",
    "    #                                              power_iterations = 3)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    #next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(32, kernel_initializer = init),\n",
    "    #                                              power_iterations = 3)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    #next_layer = tf.keras.layers.Dropout(0.1)(next_layer)\n",
    "    \n",
    "    # Бутылочное горлышко.\n",
    "    next_layer = tfa.layers.SpectralNormalization(tf.keras.layers.Dense(dimension),\n",
    "                                                  power_iterations = 3)(next_layer)\n",
    "    bottleneck = tf.keras.layers.Activation('tanh', name='bottleneck')(next_layer)\n",
    "\n",
    "    # Модель кодировщика.\n",
    "    encoder = tf.keras.Model(input_layer, bottleneck)\n",
    "\n",
    "    # Начало модели декодировщика.\n",
    "    input_code_L = tf.keras.layers.Input((dimension))\n",
    "    next_layer = input_code_L\n",
    "    \n",
    "    # 4 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(32, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 3 блок слоёв.\n",
    "    #next_layer = tf.keras.layers.Dense(128, kernel_initializer = init)(next_layer)\n",
    "    #next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 2 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(256, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "\n",
    "    # 1 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(512, kernel_initializer = init)(next_layer)\n",
    "    next_layer = tf.keras.layers.LeakyReLU(alpha=0.2)(next_layer)\n",
    "    \n",
    "    # 0 блок слоёв.\n",
    "    next_layer = tf.keras.layers.Dense(shape_input[0])(next_layer) # Подразумевается, что вход - всё равно вектор.\n",
    "    #next_layer = tf.keras.layers.Activation('tanh')(next_layer)\n",
    "    \n",
    "    output_layer = next_layer\n",
    "    \n",
    "    # Модель.\n",
    "    decoder = tf.keras.models.Model(input_code_L, output_layer) # Декодировщик.\n",
    "    autoencoder = tf.keras.Sequential([encoder, decoder])\n",
    "\n",
    "    # Компиляция модели.\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 2e-3)\n",
    "    autoencoder.compile(loss = 'mse', optimizer = opt)\n",
    "    \n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_L_autoencoder = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_L_autoencoder:\n",
    "    encoder_L = tf.keras.models.load_model(full_path + \"encoder_L.h5\")\n",
    "    decoder_L = tf.keras.models.load_model(full_path + \"decoder_L.h5\")\n",
    "    autoencoder_L = tf.keras.Sequential([encoder_L, decoder_L])\n",
    "    autoencoder.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), loss_weights = [1.0])\n",
    "\n",
    "    with open(full_path + 'info.json', 'r') as fp:\n",
    "        info = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_L_autoencoder:\n",
    "    encoder_L, decoder_L, autoencoder_L = dense_autoencoder((ds_train_L.shape[1],), codes_dim_L)\n",
    "    \n",
    "    autoencoder_L.fit(\n",
    "    ds_train_L,\n",
    "    ds_train_L,\n",
    "    epochs=autoencoders_epochs,\n",
    "    validation_data=(ds_test_L, ds_test_L),\n",
    "    batch_size=ds_train_L.shape[0] // 10)\n",
    "    \n",
    "    # Сохранение моделей.\n",
    "    autoencoder_L.save(full_path + \"autoencoder_L.h5\")\n",
    "    encoder_L.save(full_path + \"encoder_L.h5\")\n",
    "    decoder_L.save(full_path + \"decoder_L.h5\")\n",
    "    \n",
    "    # Сохранение информации.\n",
    "    with open(full_path + 'info.json', 'w') as fp:\n",
    "        json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPMRQmWmNjIS"
   },
   "source": [
    "### Получение кодов всех элементов набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вход классификатора\n",
    "_splitted = tf.split(tf.stack(ds_train_X), min_batch_number)\n",
    "codes_X = tf.concat([encoder_X(_splitted[i]) for i in range(min_batch_number)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksJnyPiNkhD"
   },
   "outputs": [],
   "source": [
    "# Выход слоя\n",
    "codes_L = np.array(encoder_L.predict(ds_train_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совместный датасет для входа классификатора и выхода слоя\n",
    "codes_X_L = np.concatenate((codes_X, codes_L), 1)\n",
    "codes_dim_X_L = codes_dim_L + codes_dim_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EMu1RqxN7vj"
   },
   "outputs": [],
   "source": [
    "PCA_codes_X = PCA(n_components=codes_dim_X, whiten=True)\n",
    "codes_pca_X = np.array(PCA_codes_X.fit_transform(codes_X))\n",
    "\n",
    "PCA_codes_L = PCA(n_components=codes_dim_L, whiten=True)\n",
    "codes_pca_L = np.array(PCA_codes_L.fit_transform(codes_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_pca_X_L = np.concatenate((codes_pca_X, codes_pca_L), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(pd.DataFrame(codes_pca_X_L[0:10000]), height = 2.0, aspect=1.6,\n",
    "                      plot_kws=dict(edgecolor=\"k\", linewidth=0.0, alpha=0.05, size=0.01, s=0.01),\n",
    "                      diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "fig = pp.fig\n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подсчёт взаимной информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L_mi_estimator = MutualInfoEstimator(n_jobs = global_n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L_mi_estimator.fit(codes_pca_X, codes_pca_L, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_X_L, mutual_information_X_L_error = X_L_mi_estimator.predict(codes_pca_X, codes_pca_L,\n",
    "                                                                                verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['mutual_information_X_L'] = mutual_information_X_L\n",
    "info['mutual_information_X_L_error'] = mutual_information_X_L_error\n",
    "\n",
    "# Сохранение информации.\n",
    "with open(full_path + 'info.json', 'w') as fp:\n",
    "    json.dump(info, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Y_mi_estimator = MutualInfoEstimator(n_jobs = global_n_jobs, Y_is_discrette=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_Y_mi_estimator.fit(codes_pca_L, ds_train_Y, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_L_Y, mutual_information_L_Y_error = L_Y_mi_estimator.predict(codes_pca_L, ds_train_Y.astype(np.int32),\n",
    "                                                                                verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(X;L) mutual information: %.2f ± %.2f\" % (mutual_information_X_L, mutual_information_X_L_error))\n",
    "print(\"(L;Y) mutual information: %.2f ± %.2f\" % (mutual_information_L_Y, mutual_information_L_Y_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
